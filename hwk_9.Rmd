---
title: "homework_9"
author: "Thomas W. Jones"
date: "11/1/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

For the general case of $\beta$ we start with

\[
\beta^k \leq n(\frac{1 + \beta}{2})^M
\]

Doing some algebra gives us

\begin{align*}
& \implies k\log_2(\beta) \leq \log_2(n) + M\log_2(\frac{1 + \beta}{2}) \\
& \implies -k\log_2(\beta) \geq -\log_2(n) -M\log_2(\frac{1 + \beta}{2}) \\
& \implies k\log_2(\frac{1}{\beta}) \geq -\log_2(n) + M\log_2(\frac{2}{1 + \beta}) \\
& \implies M\log_2(\frac{2}{1 + \beta}) \leq k\log_2(\frac{1}{\beta}) + \log_2(n) \\
& \implies M \leq \frac{k\log_2(\frac{1}{\beta}) + \log_2(n)}{\log_2(\frac{2}{1 + \beta})}
\end{align*}

The last lign of which is the inequality we are looking for.


## Problem 2

### Overview
As instructed, I implemented the k-nearest-neighbors (kNN) algorithm on the digit recognition data set. I executed this for $k \in \{1, 2, ..., 7, n\}$ for both weighted and unweighted runs, where $n$ is the total number of data points. For the weighted runs, the weights follow the formula $w_i = \frac{1}{d(x_i, x_j)^2 + 1}$ where $d(x_i, x_j)$ is the Euclidean distance between points $x_i$ and $x_j$. (I was not required to do the unweighted version for all $n$ points. However, I did so because it was easy to do in my loop, though its results are generally meaningless.)

The error rates of each run are in the table below: 

```{r, echo = FALSE}
rm(list = ls())

load("kNN_out.RData")

knitr::kable(errors)
```

On the whole, kNN works well for the digit recognition task. It's the best we have used so far! In general, the weighted version is marginally better than the unweighted version. Where $k = 1$ the result is, of course, identical. Where $k = n$, the result is poor. This is no surprise in the unweighted case. In the weighted case, I think a weight that has a stronger decay function would improve things. However, with these results, in practice I'd stop with $k = 1$.


### Treatment of the data

I applied the same treatment to the data as in the midterm. 

### Structure of the code

The code has the following main sections. 

1. Preparation of the workspace
2. Load and prepare the training data
3. Declare functions for learning and predicting a kNN classifier
4. Load and prepare the test data
5. Iterate over $k \in \{1, 2, ..., 7, n\}$ calculating kNN weighted and unweighted
6. Calculate error rates

I also coded my own euclidean distance function in C++. This function takes two matrices as arguments and calculates the pairwise distances between the rows of the two matrices. This saves on both memory and computation time over off-the-shelf distance functions. These off-the-shelf versions tend to only calculate the pairwise distance between all rows of one matrix. To use these, I'd have to create a large matrix, combining both sets of rows and discard half of the calculations. Very inefficient. 

I combined this C++ function with an R wrapper. The R wrapper trades some memory for speed. It takes the first matrix, chunks it into 4 batches. Then, the distance between each batch and the whole second matrix is calculated in parallel (at least on Unix-like systems). This should speed up the calculation time by 4, minus overhead. Though it does require making 4 copies of the entire second matrix. In this case, the data are large enough to make the tradeoff worth it. Though on smaller data, the parallel overhead would certainly not be justified.

I constructed my kNN training function very simply. "Training" in this case is simply data formatting. All features in $X$ where less than 1% of instances have a non-zero entry are discarded. (This trades some accuracy for computational efficiency.) The target, $Y$, is re-formatted from a factor (categorical) data type to be a matrix with $c$ columns, one for each class. The $i,j$ entries of this matrix are $1$ if the instance is of that class and $0$ otherwise.

The main calculation comes in the prediction function. First, it calculates pairwise distances between the test and training data as described above. This returns a matrix, $D$, of distances whose rows are the points in the test set and columns are the points in the training set. Then, the function iterates over the rows of $D$. For each row, $d$, it selects the $k$ nearest neighbors. Then it consults the rows of $Y$ and takes an average (weighted or unweighted, depending on options) of the classes for those $k$ points. The function returns a matrix whose rows correspond to the test instances and columns correspond to each class. Each row sums to 1 and represents the proportion (weighted or unweighted) of k-nearest-neighbors in each class. Hard class assignments are taken by assigning the class with the highest share.

### Instructions on running the code
Running this code will be more complicated than the previous assignments as I included a C++ function. This requires that the C++ function be stored in a directory called "scripts" just below the working directory. It must be named "DistMat2Mat.cpp". As with before, I've included code that will automatically download and install necessary R packages. However, to compile the C++ code, you must have a C++ compiler. If you are running a Windows operating system, you will also need to download and install Rtools (https://cran.r-project.org/bin/windows/Rtools). R's Rcpp package will handle compilation and integration with R. You only need to worry about the system requirements.

As before, it will also run in parallel in some sections by default if run in a Unix-like environment. I wrote and tested the below code using R version 3.4.2 on macOS Sierra 10.12.6. It should run just fine on most operating systems, including Windows. I recommend using the RStudio IDE, but it is not necessary. I am happy to provide more specific instructions if I know your operating system.

To run the code, do the following:

1. Create a folder called `data_raw` as a subfolder of your working directory.
2. Paste `data.zip` inside of `data_raw`.
3. Create a folder called `scripts` as a subfolder of your working directory.
4. Open a text editor. Paste the C++ code at the very bottom of this script inside. Save it as `scripts/DistMat2Mat.cpp`.
3. Open an R console or RStudio console.
4. If necessary, switch to your working directory using the `setwd` function.
5. Copy and paste the below code into the R console and press "enter".


```{r, eval = FALSE}
### prepare the workspace ------------------------------------------------------

# make sure my workspace is clean
rm(list = ls()) 

# make sure any necessary packages are installed
packages <- c("stringr", "pdist", "Rcpp")

lapply(packages, function(p){
  if( ! p %in% installed.packages()[ , 1 ])
    install.packages(p)
})

### load and clean the training data -------------------------------------------

# unzip the training and testing data
unzip("data_raw/data.zip", exdir = "data_raw")

# declare a "preparation" function

Prepare <- function(filepath){
  cat(filepath, "\n")
  
  # read the data in as text, row-by-row
  data <- scan(filepath, what = "character", sep = "\n")
  
  # reformat into a numeric matrix
  data <- stringr::str_split(string = data, pattern = " ")
  
  data <- do.call(rbind, lapply(data, as.numeric))
  
  # return the numeric matrix
  data
}

# iteratively load each file and prepare it
file_paths <- list.files(path = "data_raw", 
                         pattern = "^train[0-9]+\\.txt$", 
                         full.names = TRUE)

training_data <- parallel::mclapply(X = file_paths, 
                                    FUN = Prepare, 
                                    mc.cores = min(4, parallel::detectCores(), na.rm = TRUE))
# combine into a single matrix
training_data <- do.call(rbind, training_data)

# split off Y (outcome) and X (features) and further format for training
Prepare2 <- function(data) {
  
  y <- factor(data[ , 1 ])
  
  x <- data.frame(data[ , -1 ],
                  stringsAsFactors = FALSE)
  
  x <- as.data.frame(lapply(x, as.numeric))
  
  # this normalizes the rows
  x <- x / rowSums(x) 
  
  x <- as.data.frame(x)
  
  # return a list with y and x
  list(y = y, x = x)
  
}

training_data <- Prepare2(data = training_data)

### declare functions for learning and predicting ------------------------------

### This function calculates distance between the rows of two matrices
# I wrote it in C++ and compile it here. C++ code is at the bottom of this report
Rcpp::sourceCpp("scripts/DistMat2Mat.cpp")

CalcDist <- function(x, y){
  # runs in parallel on lunux/unix systems with 4 cpus
  
  x <- as.matrix(x)
  y <- as.matrix(y)
  
  b_size <- round(nrow(x) / 4)
  
  batches <- seq(1, nrow(x), by = b_size)
  
  batches <- lapply(batches[ 1:4 ], function(b){
    x[ b:min(b + b_size - 1, nrow(x)) , ]
  })
  
  checksum <- sum(sapply(batches, nrow))
  
  if (checksum < nrow(x))
    batches[[ length(batches) ]] <- rbind(batches[[ length(batches) ]],
                                          x[ (checksum + 1):nrow(x) , ])
  
  result <- parallel::mclapply(batches, function(z){
    DistMat2Mat(A = z, B = y)
  }, mc.cores = 4)
  
  result <- do.call(rbind, result)
  
  result # return this 
}

### This function trains a KNN classifier
FitKnn <- function(y, x){
  
  # format y as a n X c binary matrix
  Y <- data.frame(y = y)
  
  Y <- model.matrix(~y, Y)
  
  colnames(Y) <- levels(y)
  
  Y[ , 1 ] <- as.numeric(rowSums(Y[ , -1 ]) == 0) # fix weird formatting issue
  
  # remove columns from x where there is less than 1 % of observations with nonzero values
  x <- x[ , colSums(x > 0) > nrow(x) / 100 ]
  
  ### Because all calculation is actually done in the prediction ----
  # just assemble the training points and class in a list
  
  result <- list(training_points = x,
                 training_classes = Y)
  
  class(result) <- "kNNClassifier"
  
  result # return this value
}

# this declares a predict method for the object returned from the above function
predict.kNNClassifier <- function(object, newdata, K, 
                                  training = FALSE) {
  
  if (class(object) != "kNNClassifier")
    stop("object must be of class kNNClassifier")
  
  # get distance from all points in newdata to points in training data
  if (training) { # if this is the special case of in-sample training of the model
    
    d <- dist(newdata)
    
  } else {
    
    d <- CalcDist(x = newdata[ , colnames(object$training_points) ],
                  y = object$training_points)
    
  }
  
  
  # get variables out of our object
  Y <- object$training_classes
  
  # do the calculations for all k specified, weighted and unweighted
  result <- lapply(K, function(k){
    out <- c(weighted = TRUE, unweighted = FALSE)
    out <- lapply(out, function(weighted){
      ### two cases: k is infinite (use all points) or use only a subset ----
      
      if (is.infinite(k)) {
        
        if(weighted){
          
          weights <- 1 / (d ^ 2 + 1)
          
        } else {
          
          weights <- matrix(1, nrow = nrow(d), ncol = ncol(d))
          
          rownames(weights) <- rownames(d) 
          colnames(weights) <- colnames(d)
          
        }
        
        predictions <- weights %*% Y
        
        predictions <- predictions / rowSums(predictions)
        
        predictions  # return this value
        
      } else {
        # make our predictions
        predictions <- apply(d, 1, function(z){
          
          # get a list of candidates by returning indicies of points within k-NN
          i <- which(z <= sort(z)[ k ])
          
          # get the class values of those points
          yhat <- Y[ i , ]
          
          if (! is.matrix(yhat)) 
            yhat <- matrix(yhat, ncol = ncol(Y))
          
          # vote, either weighted or unweighted
          if ( weighted ) {
            w <- 1 / (z[ i ] ^ 2 + 1)
            
            yhat <- w %*% yhat
            
            yhat <- yhat / sum(yhat)
            
          } else {
            
            yhat <- colSums(yhat) / sum(yhat)
            
          }
          
          yhat # return this value
        })
        
        predictions <- t(predictions)
        
        colnames(predictions) <- colnames(Y)
        
        predictions # return this value
        
      }
    })
    
    out # return this value
  })
  
  names(result) <- as.character(K)
  
  result # return this value
}

### load and clean the test data -----------------------------------------------
file_paths <- list.files("data_raw", pattern = "^test[0-9]+.txt$", 
                         full.names = TRUE)

test_data <- parallel::mclapply(X = file_paths,
                                FUN = Prepare,
                                mc.cores = min(4, parallel::detectCores(), na.rm = TRUE))

test_data <- do.call(rbind, test_data)

test_data <- Prepare2(data = test_data)

### Iterate over $k = \{1, 2, ..., 7, \infty\}$ ----
# train and test

# Take a sample of 20,000 instances for training the networks
training_sample <- sample(seq_along(training_data$y), 20000)

model <- FitKnn(y = training_data$y[ training_sample ], 
                x = training_data$x[ training_sample , ])

pred <- predict(object = model, newdata = test_data$x,
                K = c(1:7, Inf))

### Calculate error rates ----

# go from probabilities to hard assignments
assignments <- parallel::mclapply(pred, function(p){
  out <- lapply(p, function(x){
    apply(x, 1, function(y) names(y)[ which.max(y) ][ 1 ])
  })
}, mc.cores = 4)

# calculate error rates
errors <- lapply(assignments, function(a){
  sapply(a, function(x){
    sum(x != test_data$y, na.rm = TRUE) / length(x)
  })
})

errors <- do.call(rbind, errors)

save(errors, assignments, pred, model, file = "kNN_out.RData")
```

```
#include <Rcpp.h>
#include <cmath>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix DistMat2Mat(NumericMatrix A, NumericMatrix B){
  
  NumericMatrix A2 = A; // don't overwrite inputs
  NumericMatrix B2 = B;
  
  int rows = A2.nrow();
  int cols = A2.ncol();
  
  int rows_b = B2.nrow();

  NumericMatrix answer(rows,rows_b);
  
  // Do the main calculations
  for(int j = 0; j < rows; j++){
    
    for(int k = 0; k < rows_b; k++){
      
      double result = 0.0;
      
      for(int i = 0; i < cols; i++){
        result = result + 
          (double(A2(j, i)) - double(B2(k, i))) *
          (double(A2(j, i)) - double(B2(k, i)));
      }
      
      answer(j , k) = std::sqrt(result);
      
    }
  }
  
  return(answer);
}
```